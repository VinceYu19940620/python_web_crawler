{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptt.html saved\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://www.ptt.cc/bbs/NBA/index.html\"\n",
    "response = requests.get(url)\n",
    "#print(reponse.text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"ptt.html\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    print(\"ptt.html saved\")\n",
    "else:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反爬蟲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptt.html saved\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "url = \"https://www.ptt.cc/bbs/NBA/index.html\"\n",
    "headers = {\n",
    "    \"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "}\n",
    "response = requests.get(url, headers = headers)\n",
    "#print(reponse.text)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(\"ptt.html\", \"w\", encoding='utf-8') as f:\n",
    "        f.write(response.text)\n",
    "    print(\"ptt.html saved\")\n",
    "else:\n",
    "    print(\"error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BeautifulSoup 解析 to JSON and excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ptt_nba_data.json saved\n",
      "ptt_nba_data.xlsx saved\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.ptt.cc/bbs/NBA/index.html\"\n",
    "headers = {\n",
    "    \"User-Agent\":'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36'\n",
    "}\n",
    "response = requests.get(url, headers = headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "articles = soup.find_all(\"div\", class_ = \"r-ent\")\n",
    "data_list = []\n",
    "\n",
    "for a in articles:\n",
    "    data = {}\n",
    "    title = a.find(\"div\", class_ = \"title\")\n",
    "    if title and title.a:\n",
    "        title = title.a.text\n",
    "    else:\n",
    "        title = \"no title\"\n",
    "    data[\"title\"] = title\n",
    "\n",
    "    popular = a.find(\"div\", class_ = \"nrec\")\n",
    "    if popular and popular.span:\n",
    "        popular = popular.span.text\n",
    "    else:\n",
    "        popular = \"N/A\"\n",
    "    data[\"popular\"] = popular\n",
    "\n",
    "    date = a.find(\"div\", class_ = \"date\")\n",
    "    if date:\n",
    "        date = date.text\n",
    "    else:\n",
    "        date = \"N/A\"\n",
    "    data[\"date\"] = date\n",
    "\n",
    "    data_list.append(data)\n",
    "    #print(f\"title: {title}, popular: {popular}, date: {date}\")\n",
    "\n",
    "#print(data_list)\n",
    "with open(\"ptt_nba_data.json\", \"w\", encoding='utf-8') as f:\n",
    "    json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "print(\"ptt_nba_data.json saved\")\n",
    "\n",
    "df = pd.DataFrame(data_list)\n",
    "df.to_excel(\"ptt_nba_data.xlsx\", index = False, engine = \"openpyxl\")\n",
    "print(\"ptt_nba_data.xlsx saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cookie 爬蟲 & image download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_img(url, save_path):\n",
    "    response = requests.get(url)\n",
    "    print(f\"download {url} to {save_path}\")\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)   \n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://i.imgur.com/aQBvKrR.jpg to images/[正妹] 禎禎與倩倩/aQBvKrR.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/Saebelr.jpg to images/[正妹] 禎禎與倩倩/Saebelr.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/MMnunW1.jpg to images/[正妹] 禎禎與倩倩/MMnunW1.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/bFtTiXN.jpg to images/[正妹] 禎禎與倩倩/bFtTiXN.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/WSdytxc.jpg to images/[正妹] 禎禎與倩倩/WSdytxc.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/42nDSq9.jpg to images/[正妹] 禎禎與倩倩/42nDSq9.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/tona9mj.jpg to images/[正妹] 禎禎與倩倩/tona9mj.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/GgK3Sq3.jpg to images/[正妹] 禎禎與倩倩/GgK3Sq3.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/TSZd2D0.jpg to images/[正妹] 禎禎與倩倩/TSZd2D0.jpg\n",
      "--------------------------------------------------\n",
      "download https://i.imgur.com/T1JsG6m.jpg to images/[正妹] 禎禎與倩倩/T1JsG6m.jpg\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "url = \"https://www.ptt.cc/bbs/Beauty/M.1708433729.A.A99.html\"\n",
    "headers = {\"Cookie\":\"over18=1\"}\n",
    "response = requests.get(url, headers = headers)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#print(soup.prettify())\n",
    "spans = soup.find_all(\"span\", class_ = \"article-meta-value\")\n",
    "title = spans[2].text\n",
    "dir_name = f\"images/{title}\"\n",
    "\n",
    "os.makedirs(dir_name, exist_ok = True) #create folder if not exist\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "allow_file_name = [\"jpg\", \"jpeg\", \"png\"]\n",
    "for link in links:\n",
    "    href = link.get(\"href\")\n",
    "    if not href:\n",
    "        continue\n",
    "    file_name = href.split(\"/\")[-1]\n",
    "    extension = href.split(\".\")[-1].lower()\n",
    "    #print(extension)\n",
    "    if extension in allow_file_name:\n",
    "        #print(f\"file name = :{extension}\")\n",
    "        #print(f\"url: {href}\")\n",
    "        download_img(href, f\"{dir_name}/{file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
